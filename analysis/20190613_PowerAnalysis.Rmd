---
title: "20190613_PowerAnalysis"
author: "Ben Fair"
date: "6/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-libraries, message=F, warning=F}
library(tidyverse)
library(knitr)
library("edgeR")
library(corrplot)
library(gplots)
library(pROC)
library(qvalue)
library(reshape2)

```

39 Chimp heart RNA-seq datasets (from this project as well as from Pavlovic et al 2018) as well as 10 human heart RNA-seq datasets (from Pavlovic et al) and 39 randomly selected GTEx left ventricle heart RNA-seq datasets were trimmed to same read length (single end, 75bp) and aligned to the respective genomes. Gene counts were obtained with subread software using gene annotations based only on orthologous exons (Pavlovic et al 2018). Here I will perform differential gene expression analysis to understand the relationship between read depth and number of individuals (samples) needed to identify cross-species differentially expressed genes.

```{r read-data}
CountTableChimp <- read.table(gzfile('../data/PowerAnalysisCountTable.Chimp.subread.txt.gz'), header=T, check.names=FALSE, skip=1)
colnames(CountTableChimp) <- paste0("C.", colnames(CountTableChimp))
kable(CountTableChimp[1:10,1:10])

CountTableHuman <- read.table(gzfile('../data/PowerAnalysisCountTable.Human.subread.txt.gz'), header=T, check.names=FALSE, skip=1)
colnames(CountTableHuman) <- paste0("H.", colnames(CountTableHuman))
kable(CountTableHuman[1:10,1:10])

CombinedTable <- inner_join(CountTableChimp[,c(1,7:length(CountTableChimp))], CountTableHuman[,c(1,7:length(CountTableHuman))], by=c("C.Geneid"="H.Geneid")) %>%
  column_to_rownames("C.Geneid") %>% as.matrix()

#Plot depth per sample
CombinedTable %>% colSums() %>% as.data.frame() %>%
  rownames_to_column("Sample") %>%
  mutate(Species=substr(Sample, 1,1)) %>%
  ggplot(aes(x=Sample, y=., fill=Species)) +
  geom_col() +
  scale_y_continuous(expand = c(0, 0)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size=6))
  

cpm <- cpm(CombinedTable, log=TRUE, prior.count=0.5)
kable(cpm[1:10,1:10])

# Heatmap of correlation. Species segregate as expected.
SpeciesFactor <- colnames(cpm) %>% substr(1,1) %>% factor() %>% unclass() %>% as.character()

cor(cpm, method = c("spearman")) %>%
heatmap.2(trace="none", ColSideColors=SpeciesFactor)
```
Unsurprisingly, the samples with the lowest read depth in the human cohort are clear outliers. This might change once I filter out the more lowly expressed genes.


```{r}
d0 <- DGEList(CombinedTable)

#Calculate normalization factors
d0 <- calcNormFactors(d0)

#Note: calcNormFactors doesnâ€™t normalize the data, it just calculates normalization factors for use downstream.
#Filter low-expressed genes

cutoff <- 2
drop <- which(apply(cpm(d0), 1, max) < cutoff)
d <- d0[-drop,] 
dim(d) # number of genes left

plotMDS(d, col = as.numeric(SpeciesFactor))
plotMDS(d, col = as.numeric(SpeciesFactor), dim=c(3,4))

cor(cpm(d, log=T, prior.count=0.5), method = c("spearman")) %>%
heatmap.2(trace="none", ColSideColors=SpeciesFactor)

mm <- model.matrix(~0 + SpeciesFactor)
y <- voom(d, mm, plot = T, normalize.method="cyclicloess")

```

In fact some of these samples seemed to have gotten worse. I'll just throw these out of future analysis.

Anway, now that voom calculated mean/variance relationship from count values, I will now normalize for gene length differences between species (Convert cpm to rpkm). As per the voom publication (Law et al 2014) log-cpm values output by voom can be converted to log-rpkm by subtracting the log2 gene length in kilobases. For this I will make a matrix of gene lengths based on chimp orthologous exon length and human orthologous exon lengths to subtract the correct length for each species from the count matrix output by voom.


```{r}
rep.col<-function(x,n){
   matrix(rep(x,each=n), ncol=n, byrow=TRUE)
}

GeneLengths <- inner_join(CountTableChimp[,c("C.Geneid", "C.Length")], CountTableHuman[,c("H.Geneid", "H.Length")], by=c("C.Geneid"="H.Geneid"))
head(kable(GeneLengths))

ggplot(GeneLengths, aes(x=log10(C.Length), y=log10(H.Length))) + geom_point()
# accounting for the length differences probably will have negligble effect on results anyway. # Will probably calculate DE genes both ways (cpm and rpkm) to verify

GeneLengthMatrix <- cbind(
  rep.col(log2(GeneLengths$C.Length/1000), length(CountTableChimp)-6),
  rep.col(log2(GeneLengths$H.Length/1000), length(CountTableHuman)-6))
rownames(GeneLengthMatrix) <- GeneLengths$C.Geneid
kable(GeneLengthMatrix[1:10,1:10])

#subtract gene log2(kb) from log2(cpm)
y$E <- y$E - GeneLengthMatrix[rownames(y$E),]

#Now do model fitting, significance testing
fit<- lmFit(y, mm)
plotSA(fit)
head(coef(fit))
head(y$E[,1])

contr <- makeContrasts(DE=SpeciesFactor1-SpeciesFactor2, levels = mm)

tmp <- contrasts.fit(fit, contrasts=contr)
FC.NullInterval <- log2(1.0)
True.efit <- treat(tmp, lfc = FC.NullInterval)

summary(decideTests(True.efit))
TrueResponse <- decideTests(True.efit)
plotMD(True.efit, column=1, zero.weights = F)
```

Ok, seems like the above workflow for identifying DE genes is set up reasonably... Now let's repeat with less samples and see what changes. First, I will wrap all of the above analysis into a function with a sample size parameter:


```{r}
DE.Subsampled <-function(ChimpCountTableFile, HumanCountTableFile, SubsampleSize, FC.NullInterval, drop)
  #if SubsampleSize parameter == 0, use full table, otherwise, subsample from it
  {
   FullChimpData <- read.table(gzfile(ChimpCountTableFile), header=T, check.names=FALSE, skip=1)
   FullHumanData <- read.table(gzfile(HumanCountTableFile), header=T, check.names=FALSE, skip=1)

   if (SubsampleSize==0){
     CountTableChimp <- FullChimpData
     colnames(CountTableChimp) <- paste0("C.", colnames(CountTableChimp))
     CountTableHuman <- FullHumanData
     colnames(CountTableHuman) <- paste0("H.", colnames(CountTableHuman))
     
   } else {
     CountTableChimp <- FullChimpData %>% dplyr::select(c(1:6, sample(7:length(FullChimpData), SubsampleSize)))
     colnames(CountTableChimp) <- paste0("C.", colnames(CountTableChimp))

     CountTableHuman <- FullHumanData %>% dplyr::select(c(1:6, sample(7:length(FullHumanData), SubsampleSize)))
     colnames(CountTableHuman) <- paste0("H.", colnames(CountTableHuman))
   }
   


CombinedTable <- inner_join(CountTableChimp[,c(1,7:length(CountTableChimp))], CountTableHuman[,c(1,7:length(CountTableHuman))], by=c("C.Geneid"="H.Geneid")) %>%
  column_to_rownames("C.Geneid") %>% as.matrix()

SpeciesFactor <- colnames(CombinedTable) %>% substr(1,1) %>% factor() %>% unclass() %>% as.character()

d0 <- DGEList(CombinedTable)
d0 <- calcNormFactors(d0)
d <- d0[-drop,] 
mm <- model.matrix(~0 + SpeciesFactor)
y <- voom(d, mm, normalize.method="cyclicloess", plot=F)

GeneLengths <- inner_join(CountTableChimp[,c("C.Geneid", "C.Length")], CountTableHuman[,c("H.Geneid", "H.Length")], by=c("C.Geneid"="H.Geneid"))
GeneLengthMatrix <- cbind(
  rep.col(log2(GeneLengths$C.Length/1000), length(CountTableChimp)-6),
  rep.col(log2(GeneLengths$H.Length/1000), length(CountTableHuman)-6))
rownames(GeneLengthMatrix) <- GeneLengths$C.Geneid
y$E <- y$E - GeneLengthMatrix[rownames(y$E),]

fit<- lmFit(y, mm)
contr <- makeContrasts(DE=SpeciesFactor1-SpeciesFactor2, levels = mm)
tmp <- contrasts.fit(fit, contrasts=contr)
efit <- treat(tmp, lfc = FC.NullInterval)
return(efit)
}

```

Now that I have a function to do all of the DE gene analysis, use the function with a smaller sample size and check results:

```{r}
#20 samples
Subsampled.DE.results <- DE.Subsampled('../data/PowerAnalysisCountTable.Chimp.subread.txt.gz',
              '../data/PowerAnalysisCountTable.Human.subread.txt.gz',
              20, 0, drop)
summary(decideTests(Subsampled.DE.results))

RocCurveData <- coords(roc(response=as.vector(abs(TrueResponse)), predictor=Subsampled.DE.results$p.value, plot=F))
plot(1-RocCurveData["specificity",], RocCurveData["sensitivity",])

SubSampledResponse <- decideTests(Subsampled.DE.results)

# distribution of effect sizes for true positives
hist(abs(True.efit$coefficients[TrueResponse==SubSampledResponse & SubSampledResponse!=0]), main="|effect size| of true positives")
median(abs(True.efit$coefficients[TrueResponse==SubSampledResponse & SubSampledResponse!=0]))


#5 samples
Subsampled.DE.results <- DE.Subsampled('../data/PowerAnalysisCountTable.Chimp.subread.txt.gz',
              '../data/PowerAnalysisCountTable.Human.subread.txt.gz',
              5, 0, drop)
summary(decideTests(Subsampled.DE.results))

RocCurveData <- coords(roc(response=as.vector(abs(TrueResponse)), predictor=Subsampled.DE.results$p.value, plot=F))
plot(1-RocCurveData["specificity",], RocCurveData["sensitivity",])

SubSampledResponse <- decideTests(Subsampled.DE.results)

# distribution of effect sizes for true positives
hist(abs(True.efit$coefficients[TrueResponse==SubSampledResponse & SubSampledResponse!=0]), main="|effect size| of true positives")
median(abs(True.efit$coefficients[TrueResponse==SubSampledResponse & SubSampledResponse!=0]))
```

Now I will systematically do this for varying sample sizes and make some final plots
```{r}
#Note there is some randomness in subsampling samples. So much so that sometimes it effects results wherein 4 samples might yield more DE genes than 2 if you get unluck and pick "bad" samples in the 4
set.seed(0)


#Interval for null hypothesis
FC.NullInterval <- log2(1.0)

#True results are those using all samples
True.efit <- DE.Subsampled('../data/PowerAnalysisCountTable.Chimp.subread.txt.gz',
              '../data/PowerAnalysisCountTable.Human.subread.txt.gz',
              0, FC.NullInterval, drop)

SampleSizes <- c(2,4,8,16,24,32,39)
FDRLevels <- c(0.01, 0.05, 0.1)


RocCurveDataToPlot <- data.frame()
DEGeneCountToPlot <- matrix(nrow=length(SampleSizes), ncol=length(FDRLevels))
rownames(DEGeneCountToPlot) <- SampleSizes
EffectSizesToPlot <- data.frame()

for (i in seq_along(SampleSizes)){
  paste0("processing ", SampleSizes[i])
  Results <- DE.Subsampled('../data/PowerAnalysisCountTable.Chimp.subread.txt.gz',
              '../data/PowerAnalysisCountTable.Human.subread.txt.gz',
              SampleSizes[i], FC.NullInterval, drop)
  
  RocCurveData <- as.data.frame(coords(roc(response=as.vector(abs(TrueResponse)), quiet=T, predictor=as.numeric(Results$p.value), plot=F), transpose=F))
  RocCurveData$samplesize <- SampleSizes[i]
  RocCurveDataToPlot <- rbind(RocCurveDataToPlot, RocCurveData)
  
  for (j in seq_along(FDRLevels)){
    
    SubSampledResponse <- decideTests(Results, p.value=FDRLevels[j])
    
    DEGeneCountToPlot[i,j] <- sum(table(SubSampledResponse)[c("-1","1")])
    
    if (length(table(TrueResponse==SubSampledResponse & SubSampledResponse!=0)) > 1){
      EffectSizes.df <-data.frame(EffectSizes=abs(True.efit$coefficients[TrueResponse==SubSampledResponse & SubSampledResponse!=0]), FDR=FDRLevels[j], SampleSize=SampleSizes[i])
      EffectSizesToPlot <- rbind(EffectSizesToPlot, EffectSizes.df)
    }

    
  }
}

ggplot(RocCurveDataToPlot, aes(x=1-specificity, y=sensitivity, color=factor(samplesize))) +
  geom_line() +
  theme_bw()

DEGeneCountToPlot.df<-as.data.frame(DEGeneCountToPlot)
colnames(DEGeneCountToPlot.df) <- FDRLevels
DEGeneCountToPlot.df$SampleSize <- rownames(DEGeneCountToPlot.df)
DEGeneCountToPlot.df$SampleSize
DEGeneCountToPlot.df[is.na(DEGeneCountToPlot.df)] <- 0
DEGeneCountToPlot.df %>% melt() %>%
  dplyr::rename(Number.DE.genes=value, FDR=variable) %>%
ggplot(aes(x=as.numeric(SampleSize), y=Number.DE.genes, color=FDR)) +
  geom_line() +
  theme_bw()


ggplot(EffectSizesToPlot, aes(x=factor(SampleSize), y=EffectSizes, color=factor(FDR))) +
  # geom_violin()
  geom_boxplot() +
  # scale_y_continuous(limits=c(0,5))
  theme_bw()
  
# ggplot(EffectSizesToPlot, aes(x=as.numeric(SampleSize), y=median(EffectSizes), color=factor(FDR), group=factor(FDR))) +
#   geom_line() +
#   # scale_y_continuous(limits=c(0,5))
#   theme_bw()

data.frame(coefficients=as.numeric(True.efit$coefficients), pval=as.numeric(True.efit$p.value), signif=decideTests(True.efit)) %>%
ggplot(aes(x=coefficients, y=-log10(pval), color=factor(DE))) +
  geom_point() +
  scale_x_continuous(limits=c(-5,5))
```



I probably also want to explore how read depth plays into this, especially considering some samples were much more sparse than others. Here I'll try to write a function to subsample without replacement the read counts from the original count table.
```{r, eval=F}
sample_species <- function(counts,n) {
  num_species <- length(counts)
  total_count <- sum(counts)
  samples <- sample(1:total_count,n,replace=F)
  samples <- samples[order(samples)]
  result <- array(0,num_species)
  total <- 0
  for (i in 1:num_species) {
    result[i] <- length(which(samples > total & samples <= total+counts[i]))
    total <- total+counts[i]
  }
  return(result)
  # return(apply(t(counts),1,sample_species,1500) )
}

sample_count_table <- function(counts, n){
  return(apply(t(counts),1,sample_species,n) )
}

A<- sample_count_table(CombinedTable,2200000)
```

This is still a work in progress... It seems that sampling without replcement from a large count table actually takes an unreasonably long time (and probably uses a lot of memory)... Sampling without replacement could be faster in theory, but I'm not sure if that is still reasonable. There are some third party packages out there that do this, maybe I will try them out. Though, I don't see any that let you pick a number of reads to subsample as opposed to a proportion of reads to subsample (in other words, it wouldn't be easy to normalize read depth across samples). Maybe it will actually just be easier to realign everything at various read depths and make new count table files for each read depth if I want to incorporate normalizing and adjusting read depths into this analysis.


Update: What I turned out doing is sampling from the original alignment bam file to varying depths, normalizing such that each sample has an equal number of aligned reads. Then count tables were generated (one table for chimp, one for human), and the jist of this differential expression pipeline in R was wrapped into an Rscript that takes as input the count table, and as output spits out figures for ROC curves and such for DE power after randomly subsampling the samples (columns) of the count table at various sample sizes. This script was then incorporated into the snakemake for this repo.
